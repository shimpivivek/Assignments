{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dce27b1-9c89-4d92-858b-507a101705b9",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cba205-e06a-4053-af45-3247e8b8e4ea",
   "metadata": {},
   "source": [
    "Min-Max scaling also known as normalization is a data preprocessing technique used to rescale numerical features to a specific range, typically [0, 1]. The goal of Min-Max scaling is to transform the data in such a way that it falls within a specific interval, making it more suitable for certain machine learning algorithms and improving convergence during training.\n",
    "\n",
    "X Scaled = X-Xmin/Xmax-Xmin\n",
    "\n",
    "Min-Max scaling is particularly useful when features have different units or scales, as it standardizes them to a consistent range. This can be important for machine learning algorithms like neural networks and support vector machines, which are sensitive to the scale of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445412aa-24dd-41c7-b579-0d8d68d13c56",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831403c-32d7-4fb8-9639-8202fd181f89",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Unit Length scaling or Normalization, is a feature scaling method that transforms the data such that each feature vector has a length of 1 (i.e., it converts the feature vectors into unit vectors). This is achieved by dividing each feature value by the Euclidean norm (magnitude) of the feature vector.\n",
    "\n",
    "X scaled = x / |x|\n",
    "here |x|= sq (x1^2 + x2^2 + ....... + xn^2)\n",
    "\n",
    "Differences from Min-Max Scaling:\n",
    "\n",
    "Range of Values:\n",
    "\n",
    "Min-Max scaling scales features to a specific range (typically [0, 1]), while Unit Vector scaling normalizes feature vectors to have a length of 1.\n",
    "Unit Vector vs. Single Value:\n",
    "\n",
    "Min-Max scaling operates on individual feature values, while Unit Vector scaling operates on entire feature vectors.\n",
    "Unit Vector scaling is particularly useful in scenarios where the direction of the feature vectors is more important than their magnitude. This is common in machine learning algorithms like support vector machines and k-nearest neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d6eee-5045-4eb2-ac1d-6938b737d3e5",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58444e73-7d31-4201-849f-cc8f5221fb79",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining as much of the original variance as possible. It achieves this by identifying a new set of axes, called principal components, along which the data has the highest variability.\n",
    "\n",
    "Example- Suppose you have a dataset with two features: height (in centimeters) and weight (in kilograms) of individuals. You want to reduce the dimensionality of this data while retaining as much information as possible.\n",
    "\n",
    "-Standardize the Data:\n",
    "If necessary, standardize the height and weight.\n",
    "\n",
    "-Calculate Covariance Matrix:\n",
    "Compute the covariance matrix to understand the relationships between height and weight.\n",
    "\n",
    "-Calculate Eigenvectors and Eigenvalues:\n",
    "Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "-Sort Eigenvectors by Eigenvalues:\n",
    "If the first eigenvector corresponds to height, and the second to weight, arrange them accordingly.\n",
    "\n",
    "-Select Principal Components:\n",
    "You might choose to keep only the first eigenvector (representing height) as your new dimension.\n",
    "\n",
    "-Project Data onto New Basis:\n",
    "Multiply the original data by the first eigenvector to obtain the transformed data in the lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71edacbe-1a18-4b69-b509-1f9816bf1b3a",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3437e1-2c61-4e3a-abf7-2a2e39293689",
   "metadata": {},
   "source": [
    "PCA is a technique that can be used for feature extraction. Feature extraction aims to reduce the dimensionality of a dataset while preserving as much relevant information as possible. PCA achieves this by identifying a new set of axes that capture the maximum variance in the data.\n",
    "\n",
    "In the context of feature extraction, PCA identifies a smaller set of features that retain the most important information from the original dataset. These new features are linear combinations of the original features and represent the directions of highest variance.\n",
    "\n",
    "For example, consider a dataset containing various facial features (e.g., eyes, nose, mouth) for facial recognition. PCA can be applied to extract key facial components, such as eigenfaces, which are linear combinations of the original pixel values. These eigenfaces represent the most discriminative features for facial recognition.\n",
    "\n",
    "By using PCA for feature extraction, we can effectively represent complex data in a more compact form, making it easier for machine learning algorithms to process and make accurate predictions or classifications. This is especially valuable in high-dimensional datasets where reducing feature space can lead to more efficient and accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd94409-9253-43a0-b1a6-70a000562f0f",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710e4da-4424-4146-8b76-c31dc79705e7",
   "metadata": {},
   "source": [
    "\n",
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow these steps:\n",
    "\n",
    "### 1) Understand the Features:\n",
    "\n",
    "Familiarize yourself with the dataset and the features it contains, including price, rating, delivery time, and any others relevant to the recommendation system.\n",
    "### 2) Standardization :\n",
    "\n",
    "If the features have different scales (e.g., price in rs, rating on a scale of 1 to 5), consider standardizing them to ensure they contribute equally to the analysis. This involves subtracting the mean and dividing by the standard deviation.\n",
    "### 3) Apply Min-Max Scaling:\n",
    "\n",
    "For each feature, apply the Min-Max scaling formula to scale the values to a specific range, typically [0, 1]. The formula is:\n",
    "he feature.\n",
    "\n",
    "### 4) Verify Scaled Data:\n",
    "Check the scaled values to ensure they fall within the desired range (typically [0, 1]).\n",
    "\n",
    "### 5) Use Scaled Data for Recommendation System:\n",
    "Utilize the scaled features in building the recommendation system. The scaled values will help ensure that each feature contributes appropriately to the system's recommendations.\n",
    "\n",
    "By applying Min-Max scaling, you'll effectively standardize the range of feature values, making them more compatible for use in the recommendation system. This preprocessing step helps ensure that no single feature dominates the recommendation process due to its larger scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38370377-81a8-4347-a99b-64a48707bb3e",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2a5c3-6912-4fcc-8e11-a4b4d95b2318",
   "metadata": {},
   "source": [
    "To use PCA for reducing the dimensionality of the dataset in a stock price prediction project, follow these steps:\n",
    "\n",
    "### 1) Standardize the Data:\n",
    "\n",
    "If the features have different scales , standardize them to have a mean of 0 and a standard deviation of 1. This ensures that all features contribute equally to the PCA.\n",
    "### 2) Calculate the Covariance Matrix:\n",
    "\n",
    "Compute the covariance matrix of the standardized features. This matrix summarizes the relationships between different features.\n",
    "### 3) Calculate Eigenvectors and Eigenvalues:\n",
    "\n",
    "Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance, and eigenvalues indicate the amount of variance along each eigenvector.\n",
    "### 4) Sort Eigenvectors by Eigenvalues:\n",
    "\n",
    "Arrange the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvector with the highest eigenvalue represents the direction with the highest variance.\n",
    "### 5) Select Principal Components:\n",
    "\n",
    "Choose the top k eigenvectors to form the basis for the new lower-dimensional space. These k eigenvectors are referred to as the principal components.\n",
    "### 6) Project Data onto New Basis:\n",
    "\n",
    "Multiply the original data matrix by the matrix of selected eigenvectors. This projection transforms the data into the lower-dimensional space defined by the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a3fec-293c-4f78-b93c-16c60e58aa45",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "222c3e58-8575-48e5-b385-6f2d6634825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]\n"
     ]
    }
   ],
   "source": [
    "data=[1,5,10,15,20]\n",
    "new_data=[]\n",
    "x_min=min(data)\n",
    "x_max=max(data)  \n",
    "for i in data:\n",
    "    i=(i-x_min)/(x_max-x_min)\n",
    "    new_data.append(i)\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17bc966-a1ec-423d-bbe3-478f3d1749bb",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa433b68-b363-4178-b224-10b76899c543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d843b-e161-4eed-a5c8-53c5b476c446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d752efb-f5d4-4e7f-add2-7ca5593fe9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e424742-d649-4fa4-beff-97651aba11dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d90970-36c9-4e1c-9aa0-05a60925a2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738846b-8080-47d3-b10c-ee4c16a2d50d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae7464d-4d56-4744-b406-3c4dad64ce7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bf2cc-009c-4621-9bb4-a40374db1a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cae722-e5dc-44ec-9526-6aa37a48db9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a755843-13a4-4912-b0ed-c8eebfc61520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfea86a-cddf-464c-9242-de4ee8300fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4b7a0-5d9f-4609-be9a-9821787d68c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6106dd-d50c-4bb4-b077-f882ab02706f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41b330-eab1-4254-9d09-880110d8e4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd3ec6-9184-4cab-90ce-b97800e11d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3626102-940b-452d-b3ad-d96456d11fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa9d2d-bce1-4998-8b45-37923dffb3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0dc48-9286-4fa6-aac0-fb13fc1b31a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c29d9-bb17-4004-a1b2-1b62cc389426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d86735-b5d2-4600-8b90-bf5e1e395e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53f8e2-b61c-42fe-b61f-f096c0a6431d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412804c-9b8e-4647-929c-56c56c42510a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dca30-531b-416d-8480-bc65fb4b6cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aaa5ea-d66a-420d-9841-4c1e8927765b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b409833-480f-42eb-89b5-1bab53b27c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
